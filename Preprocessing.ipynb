{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "jUpcJTkGDUDs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duaOXbURQZsM",
        "outputId": "c12e54fe-8770-4fcc-b67c-8d3dc1083bbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import sys\n",
        "import io \n",
        "import nltk\n",
        "\n",
        "#remove stopwords\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from nltk.corpus import stopwords \n",
        "nltk.download('stopwords')\n",
        "stops = set(stopwords.words('english'))\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "#add to the stopwords list some archaic forms of pronouns that may be present in some translations\n",
        "archaic = ['thou', 'thee', 'ye', 'thy', 'thine']\n",
        "stopwords.extend(archaic)\n",
        "  \n",
        "#divide text in sentences based on points\n",
        "nltk.download('punkt')\n",
        "\n",
        "#tokenize strings in words\n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "#lemmatization\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "#convert position tag in a form suitable for the lemmatizer\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "#count unique words\n",
        "from collections import Counter\n",
        "\n",
        "#explore results\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xu7YOak9dx0z",
        "outputId": "b3ad3ec6-85b0-4ab3-fa5f-a472f37845a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#os.chdir('/content/drive/MyDrive/Magistrale/Secondo semestre/DS/Progetto/HolyText_Corpora')"
      ],
      "metadata": {
        "id": "3WlsJZnFfLRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "To increase the size of the training corpora, we decide to use more translations of each considered holy book (Saeed et al., 2020). We decide how many translations to use for each holy book considering the availability of free copies online and with the goal of having similar values of total and unique words for each corpora.\n",
        "\n",
        "We preprocess the collected text with the following steps:\n",
        "- convert uppercase to lowercase;\n",
        "- remove non alphanumeric characters;\n",
        "- remove punctuation;\n",
        "- remove stop words.\n",
        "\n",
        "Since we are not sure if performing lemmatization is a good idea we save two different preprocessed versions for each corpora, one with lemmatization and one without. We will decide later if lemmatization is a good idea or not.\n",
        "\n",
        "After the preprocessing, the dimensions of the corpora are as follows:\n",
        "- with lemmatization\n",
        "\n",
        "| Religion      | Holy text | N. of translations | N.words | N.unique words |\n",
        "| ----------- | ----------- | ------------------ | ------- | -------------- |\n",
        "| Christianity| Bible       | 3 | 998432 | 14544\n",
        "| Islam   | Quran        |   8  | 784578 | 16735\n",
        "| Hinduism | Vedas and Upanishads | 2 and 2 | 687782 | 15227\n",
        "| Buddhism | Tripitaka | 8 books of the Tripitaka | 403179 | 15347\n",
        "\n",
        "- without lemmatization:\n",
        "\n",
        "| Religion      | Holy text | N. of translations | N.words | N.unique words |\n",
        "| ----------- | ----------- | ------------------ | ------- | -------------- |\n",
        "| Christianity| Bible       | 3 | 1004875 | 19157\n",
        "| Islam   | Quran        |   8  | 791238 | 22069\n",
        "| Hinduism | Vedas and Upanishads | 2 and 2 | 697742 | 18874\n",
        "| Buddhism | Tripitaka | 8 books of the Tripitaka | 404717 | 19699\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XyBhZ3CATtc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions "
      ],
      "metadata": {
        "id": "BTxka6tODp2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(file_name):\n",
        "\n",
        "  output=\"\"\n",
        "  with open(file_name, encoding = 'utf-8-sig') as f:\n",
        "      for line in f:\n",
        "          if not line.isspace():#remove empty lines\n",
        "              output+=line\n",
        "\n",
        "  #divide the output text in sentences, based on points\n",
        "  output_sentences = nltk.tokenize.sent_tokenize(output)\n",
        "\n",
        "  #remove first 100 and last 100 sentences that normally are licences, greetings, ..\n",
        "  output_sentences = output_sentences[100:-100]\n",
        "\n",
        "  filtered_sentences = []\n",
        "  #'clean' every sentence, one by one\n",
        "  for sentence in output_sentences:\n",
        "    #capital letters to lower\n",
        "    lower_sentence=sentence.lower()\n",
        "    #remove non alphanumeric characters\n",
        "    noalfa_sentence = [w for w in word_tokenize(lower_sentence) if (w.isalpha()==True)]\n",
        "    #lemmatize\n",
        "    lemmatized_sentence = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in noalfa_sentence]\n",
        "    #remove stopwords and words of one character that may not be included in the stopwords list\n",
        "    filtered_sentence = [w for w in lemmatized_sentence if ((w not in stopwords) and (len(w) > 1))]\n",
        "    #insert the filtered and tokenized sentence in the final list\n",
        "    if filtered_sentence:\n",
        "      filtered_sentences.append(filtered_sentence)\n",
        "\n",
        "  return filtered_sentences"
      ],
      "metadata": {
        "id": "p8uETXOxQ9ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing without lemmatization\n",
        "def preprocessing_nolemm(file_name):\n",
        "\n",
        "  output=\"\"\n",
        "  with open(file_name, encoding = 'utf-8-sig') as f:\n",
        "      for line in f:\n",
        "          if not line.isspace():#remove empty lines\n",
        "              output+=line\n",
        "\n",
        "  #divide the output text in sentences, based on points\n",
        "  output_sentences = nltk.tokenize.sent_tokenize(output)\n",
        "\n",
        "  #remove first 100 and last 100 sentences that normally are licences, greetings, ..\n",
        "  output_sentences = output_sentences[100:-100]\n",
        "\n",
        "  filtered_sentences = []\n",
        "  #'clean' every sentences, one by one\n",
        "  for sentence in output_sentences:\n",
        "    #capital letters to lower\n",
        "    lower_sentence=sentence.lower()\n",
        "    #remove non alphanumeric characters\n",
        "    noalfa_sentence = [w for w in word_tokenize(lower_sentence) if (w.isalpha()==True)]\n",
        "    #remove stopwords and words of one character that may not be included in the stopwords list\n",
        "    filtered_sentence = [w for w in noalfa_sentence if ((w not in stopwords) and (len(w) > 1))]\n",
        "    #insert the filtered and tokenized sentence in the final list\n",
        "    if filtered_sentence:\n",
        "      filtered_sentences.append(filtered_sentence)\n",
        "\n",
        "  return filtered_sentences"
      ],
      "metadata": {
        "id": "y4OdV35ldXV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def checkresults(sentences):\n",
        "  #explore the results\n",
        "  words = []\n",
        "  for i in range(len(sentences)):\n",
        "    for j in range(len(sentences[i])):\n",
        "      words.append(sentences[i][j])\n",
        "\n",
        "  #number of words\n",
        "  print('Words: ' + str(len(words)))\n",
        "\n",
        "  #number of unique words\n",
        "  word_count = Counter(words)\n",
        "  keys = word_count.keys()\n",
        "  print('Unique words: '+ str(len(keys)))\n",
        "\n",
        "  #most common words\n",
        "  print('Most common words:')\n",
        "  print(word_count.most_common(10))\n",
        "\n",
        "  #print some sentences\n",
        "  print('Random sentences:')\n",
        "  randomlist = random.sample(range(0, len(sentences)), 25)\n",
        "  for i in randomlist:\n",
        "    print(sentences[i])"
      ],
      "metadata": {
        "id": "1Ovx3KVfX7gH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def savesentences(filename, sentences):\n",
        "  with open(filename + '.txt', 'w') as fp:\n",
        "    for sentence in sentences:\n",
        "      fp.write(str(sentence) + '\\n')"
      ],
      "metadata": {
        "id": "aMJM0IHday6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save sentences not lemmatized\n",
        "def savesentencesnl(filename, sentences):\n",
        "  #os.chdir('/content/drive/MyDrive/Magistrale/Secondo semestre/DS/Progetto/Sentences_nl')\n",
        "  with open(filename + '_nl.txt', 'w') as fp:\n",
        "    for sentence in sentences:\n",
        "      fp.write(str(sentence) + '\\n')"
      ],
      "metadata": {
        "id": "FHYm0sVGh4YJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "hTJu8Z6dXNg4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Christianity"
      ],
      "metadata": {
        "id": "XMzSdZUhSNd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Christian_files = ['KingJamesVersion.txt', 'NIV-Bible.txt','WorldEnglishBible.txt']\n",
        "Christian_sentences = []\n",
        "Christian_sentences_nl = []\n",
        "\n",
        "for file_name in Christian_files:\n",
        "  Christian_sentences = Christian_sentences + preprocessing(file_name)\n",
        "  Christian_sentences_nl = Christian_sentences_nl + preprocessing_nolemm(file_name)"
      ],
      "metadata": {
        "id": "Za14xMsUSO4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkresults(Christian_sentences)"
      ],
      "metadata": {
        "id": "msC2PITaatfr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aca46958-0917-4af8-fa06-4005c5cbd46c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words: 1004875\n",
            "Unique words: 19157\n",
            "Most common words:\n",
            "[('shall', 17608), ('lord', 17074), ('god', 12527), ('said', 11024), ('unto', 8972), ('king', 7537), ('man', 7235), ('son', 7098), ('israel', 7009), ('yahweh', 6807)]\n",
            "Random sentences:\n",
            "['concord', 'hath', 'christ', 'belial']\n",
            "['comes', 'defy', 'israel']\n",
            "['whence', 'cometh', 'wisdom']\n",
            "['others', 'mocking', 'said', 'filled', 'new', 'wine']\n",
            "['david', 'saith', 'book', 'psalms', 'lord', 'said', 'unto', 'lord', 'sit', 'right', 'hand', 'till', 'make', 'enemies', 'footstool']\n",
            "['forgat', 'lord', 'god', 'sold', 'hand', 'sisera', 'captain', 'host', 'hazor', 'hand', 'philistines', 'hand', 'king', 'moab', 'fought']\n",
            "['struck', 'souls', 'therein', 'edge', 'sword', 'utterly', 'destroying', 'none', 'left', 'breathed', 'burnt', 'hazor', 'fire']\n",
            "['cut', 'olive', 'tree', 'wild', 'nature', 'contrary', 'nature', 'grafted', 'cultivated', 'olive', 'tree', 'much', 'readily', 'natural', 'branches', 'grafted', 'olive', 'tree']\n",
            "['shall', 'commit', 'adultery']\n",
            "['window', 'shalt', 'make', 'ark', 'cubit', 'shalt', 'finish', 'door', 'ark', 'shalt', 'set', 'side', 'thereof', 'lower', 'second', 'third', 'stories', 'shalt', 'make']\n",
            "['fear']\n",
            "['violently', 'taken', 'away', 'tent', 'garden', 'destroyed', 'place', 'assembly', 'yahweh', 'caused', 'solemn', 'assembly', 'sabbath', 'forgotten', 'zion', 'despised', 'indignation', 'anger', 'king', 'priest']\n",
            "['came', 'rome']\n",
            "['men', 'david', 'sware', 'unto', 'saying', 'shalt', 'go', 'us', 'battle', 'quench', 'light', 'israel']\n",
            "['unless', 'lord', 'shortened', 'days', 'flesh', 'would', 'saved', 'sake', 'chosen', 'ones', 'picked', 'shortened', 'days']\n",
            "['david', 'said', 'servants', 'jerusalem', 'arise', 'let', 'us', 'flee', 'else', 'none', 'us', 'shall', 'escape', 'absalom', 'make', 'speed', 'depart', 'lest', 'overtake', 'us', 'quickly', 'bring', 'evil', 'us', 'strike', 'city', 'edge', 'sword']\n",
            "['one', 'speaks', 'rashly', 'like', 'piercing', 'sword', 'tongue', 'wise', 'heals']\n",
            "['men', 'beth', 'shemesh', 'said', 'able', 'stand', 'yahweh', 'holy', 'god']\n",
            "['therefore', 'consider']\n",
            "['happened', 'man', 'god', 'spoken', 'king', 'saying', 'two', 'measures', 'barley', 'shekel', 'measure', 'fine', 'flour', 'shekel', 'shall', 'tomorrow', 'time', 'gate', 'samaria', 'captain', 'answered', 'man', 'god', 'said', 'behold', 'yahweh', 'make', 'windows', 'heaven', 'might', 'thing']\n",
            "['sailed', 'across', 'sea', 'cilicia', 'pamphylia', 'came', 'myra', 'city', 'lycia']\n",
            "['john', 'beginning', 'heard', 'seen', 'eyes', 'looked', 'hands', 'proclaim', 'concerning', 'word', 'life']\n",
            "['alexander', 'metalworker', 'great', 'deal', 'harm']\n",
            "['church', 'antioch', 'prophets', 'teachers', 'barnabas', 'simeon', 'called', 'niger', 'lucius', 'cyrene', 'manaen', 'brought', 'herod', 'tetrarch', 'saul']\n",
            "['baptized', 'baptism', 'baptized']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkresults(Christian_sentences_nl)"
      ],
      "metadata": {
        "id": "e_pYHuGNGUNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "savesentences('Christian_sentences',Christian_sentences)"
      ],
      "metadata": {
        "id": "G36dU2BRGW2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "savesentencesnl('Christian_sentences',Christian_sentences)"
      ],
      "metadata": {
        "id": "JucXOXbBbM76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Islam\n",
        "\n"
      ],
      "metadata": {
        "id": "UjHH9QB1Vc7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Islam_files =  ['clearquran.txt','habib-shakir.txt','marmaduke.txt','quranalhilali-khan.txt', 'sarwar.txt','Quran-Saheeh.txt', 'yaq.txt','YusufAli.txt']\n",
        "Islam_sentences = []\n",
        "Islam_sentences_nl = []\n",
        "\n",
        "for file_name in Islam_files:\n",
        "  Islam_sentences = Islam_sentences + preprocessing(file_name)\n",
        "  Islam_sentences_nl = Islam_sentences_nl + preprocessing_nolemm(file_name)\n",
        "  "
      ],
      "metadata": {
        "id": "aOUowzCtVyQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkresults(Islam_sentences)"
      ],
      "metadata": {
        "id": "CHFXK7QqbXxU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b70d5aa5-674e-43d1-a758-444860fbc301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words: 791238\n",
            "Unique words: 22069\n",
            "Most common words:\n",
            "[('allah', 18177), ('god', 10529), ('lord', 8142), ('said', 6920), ('people', 6299), ('say', 6188), ('one', 5049), ('day', 4746), ('shall', 4524), ('us', 4502)]\n",
            "Random sentences:\n",
            "['cf']\n",
            "['see', 'footnote']\n",
            "['said', 'indeed', 'sent', 'people']\n",
            "['believe', 'good', 'good', 'final', 'state', 'shall', 'goodly', 'return']\n",
            "['unbelievers', 'hope']\n",
            "['appoint', 'henchman', 'folk', 'aaron', 'brother']\n",
            "['thought', 'harm', 'would', 'come', 'willfully', 'blind', 'deaf']\n",
            "['name', 'ofallah', 'gracious', 'merciful']\n",
            "['never', 'compass', 'anything', 'knowledge', 'except', 'wills']\n",
            "['cf']\n",
            "['let', 'present', 'life', 'deceive', 'let', 'chief', 'deceiver', 'deceive', 'allah']\n",
            "['may', 'either', 'time', 'noontide', 'siesta', 'business', 'suspended', 'even', 'egypt', 'time', 'night', 'people', 'usually', 'asleep']\n",
            "['cf']\n",
            "['lodge', 'section', 'dwell', 'means', 'harm', 'order', 'oppress', 'pregnant', 'spend', 'give', 'birth']\n",
            "['decree', 'already', 'recorded', 'book', 'issuing', 'judgment', 'difficult']\n",
            "['simile', 'apt', 'stunned', 'beings', 'rise', 'swarms', 'graves', 'say']\n",
            "['feed', 'sake', 'allah']\n",
            "['seen', 'given', 'portion', 'scripture', 'believe', 'jibt', 'superstition', 'false', 'objects', 'worship', 'say', 'disbelievers', 'better', 'guided', 'believers', 'way']\n",
            "['commencement', 'hajj', 'days', 'enters', 'state', 'lhrdm', 'performs', 'hajj']\n",
            "['gardens', 'beneath', 'flow', 'rivers']\n",
            "['working', 'allah', 'providence', 'clearly', 'visible', 'story', 'quraysh']\n",
            "['fact', 'understanding', 'except']\n",
            "['turn', 'away']\n",
            "['hypocrites', 'inconsistent', 'reflect', 'unregenerate', 'mankind']\n",
            "['would', 'punished', 'disbelievers', 'world', 'previous', 'peoples']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkresults(Islam_sentences_nl)"
      ],
      "metadata": {
        "id": "o_aDiiCrGj3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "savesentences('Islam_sentences',Islam_sentences)"
      ],
      "metadata": {
        "id": "2klA_Qm8GnoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "savesentencesnl('Islam_sentences',Islam_sentences)"
      ],
      "metadata": {
        "id": "64CQOoPybbcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hinduism"
      ],
      "metadata": {
        "id": "wBwQUmEOYtZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files_Hinduism = ['paramanda_upanishads.txt','the-4-vedas.txt','17001079-Four-Vedas-English-Translation.txt', '4Upanishads - Shukla Yajur Veda.txt']\n",
        "Hinduism_sentences = []\n",
        "Hinduism_sentences_nl = []\n",
        "\n",
        "for file_name in files_Hinduism:\n",
        "  Hinduism_sentences = Hinduism_sentences + preprocessing(file_name)\n",
        "  Hinduism_sentences_nl = Hinduism_sentences_nl + preprocessing_nolemm(file_name)"
      ],
      "metadata": {
        "id": "ZEsVvSFtYvGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkresults(Hinduism_sentences)"
      ],
      "metadata": {
        "id": "WHXoL-MvberS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cf553ce-7cb5-4de8-8efe-6eaf509eff9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words: 697742\n",
            "Unique words: 18874\n",
            "Most common words:\n",
            "[('us', 9479), ('indra', 9161), ('agni', 7731), ('may', 7321), ('gods', 6141), ('verily', 5121), ('soma', 5080), ('one', 4683), ('sacrifice', 4405), ('come', 4361)]\n",
            "Random sentences:\n",
            "['pusan']\n",
            "['hymn', 'cxxiii']\n",
            "['armour', 'heaven', 'earth', 'armour', 'day', 'armour', 'sun']\n",
            "['yajnavalkya', 'replied', 'one', 'wishing', 'go', 'long', 'distance', 'emperor', 'secure', 'chariot', 'boat', 'fully', 'equipped', 'mind', 'many', 'secret', 'names', 'brahman']\n",
            "['wife', 'would', 'yield', 'husband']\n",
            "['growest', 'upon', 'mountain', 'eagle', 'art', 'sprung', 'himavant', 'come', 'treasures', 'heard', 'fame']\n",
            "['indra', 'associate', 'priests', 'cleared', 'stable', 'full', 'steeds', 'kine', 'giving', 'thousand', 'eightmarked', 'cars', 'gained', 'renown', 'among', 'gods']\n",
            "['victory', 'universal', 'dharma']\n",
            "['keep', 'us', 'safely', 'spiteful', 'curse', 'presumptuous', 'foe']\n",
            "['let', 'thousands', 'slain', 'may', 'club', 'bhava', 'crush']\n",
            "['mountain', 'grows', 'poisonous', 'plants', 'rendered', 'impotent']\n",
            "['cars', 'bring', 'oblation', 'turned', 'rightward', 'swell', 'marut']\n",
            "['hail']\n",
            "['whose', 'men', 'utter', 'speech']\n",
            "['visnu']\n",
            "['brahma', 'vishnu', 'indra', 'god', 'death', 'sun', 'moon', 'gods', 'demons', 'men', 'women', 'animals', 'etc']\n",
            "['may', 'allied', 'first', 'rank', 'princes', 'obtain', 'possessions', 'exertion']\n",
            "['mantra', 'indra', 'author', 'pustigu', 'kanva']\n",
            "['pious', 'man', 'endues', 'beams', 'morning']\n",
            "['bless', 'us', 'holy', 'ones', 'may', 'help', 'guard', 'protect', 'us', 'malignant', 'injury']\n",
            "['made', 'lord', 'youthful', 'maidens']\n",
            "['mantra', 'portions', 'butter', 'two', 'portions', 'oblation', 'eyes', 'sacrifice']\n",
            "['hast', 'covered', 'heaven', 'earth', 'splendour', 'glories', 'glorious', 'triumphant']\n",
            "['art', 'lioness', 'overcoming', 'rivals', 'hail']\n",
            "['repeat', 'fifteen', 'samidhenis', 'case', 'rajanya', 'rajanya', 'fifteenfold', 'verily', 'makes', 'find', 'support', 'stoma']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkresults(Hinduism_sentences_nl)"
      ],
      "metadata": {
        "id": "QJGq31uvGxxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "savesentences('Hinduism_sentences',Hinduism_sentences)"
      ],
      "metadata": {
        "id": "o44QAYANGzkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "savesentencesnl('Hinduism_sentences',Hinduism_sentences)"
      ],
      "metadata": {
        "id": "BLh7lGI_bhEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Buddhism"
      ],
      "metadata": {
        "id": "mRkGeR3_ZrSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files_Buddhism = ['discorsi_budda.txt','discorsi_budda_2.txt','discorsi_budda_3.txt', 'budda4.txt','budda5.txt', 'BuddhistMonasticTraditionsofSouthernAsia.txt','Buddhacarita.txt', 'Brahmas_Net_Sutra.txt']\n",
        "Buddhism_sentences = []\n",
        "Buddhism_sentences_nl = []\n",
        "\n",
        "for file_name in files_Buddhism:\n",
        "  Buddhism_sentences = Buddhism_sentences + preprocessing(file_name)\n",
        "  Buddhism_sentences_nl = Buddhism_sentences_nl + preprocessing_nolemm(file_name)"
      ],
      "metadata": {
        "id": "g17xyr4VZswI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkresults(Buddhism_sentences)"
      ],
      "metadata": {
        "id": "H6Zxq63AbojX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9832d8e9-d99b-4039-f4d7-4dcbb547b006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words: 404717\n",
            "Unique words: 19699\n",
            "Most common words:\n",
            "[('one', 5275), ('buddha', 3319), ('see', 2622), ('people', 2216), ('also', 1930), ('world', 1570), ('great', 1547), ('way', 1506), ('three', 1473), ('life', 1470)]\n",
            "Random sentences:\n",
            "['sutra', 'queen', 'śrīmālā', 'lion', 'roar', 'ch']\n",
            "['nevertheless', 'among', 'establish', 'meritorious', 'acts', 'violate', 'precepts', 'minds', 'yet', 'live', 'place', 'hermitage', 'recite', 'many', 'buddhist', 'scriptures', 'chant', 'continually', 'yet', 'smoothly', 'explain', 'doctrines', 'already', 'aged', 'fall', 'three', 'classes', 'mentioned', 'yet', 'whose', 'essential', 'nature', 'pure', 'virtuous', 'serious', 'evil']\n",
            "['without', 'one', 'good', 'qualities', 'established', 'morality']\n",
            "['said', 'buddha', 'may', 'request', 'one', 'holiness', 'venerable', 'disciples', 'accept', 'tomorrow', 'meal', 'place']\n",
            "['vicikitsā']\n",
            "['turns', 'noble', 'humble', 'upside', 'falsely', 'makes', 'talk']\n",
            "['seventh', 'sentient', 'beings', 'reside', 'sphere', 'nothingness', 'nonutility', 'seventh', 'abode', 'consciousness']\n",
            "['prince', 'reṇu', 'enthroned']\n",
            "['also', 'called', 'three', 'evil', 'paths', 'san', 'edao']\n",
            "['one', 'person', 'poṣadha', 'one', 'person', 'chanting']\n",
            "['lunyu', 'xianwen', 'pi', 'chen', 'shi', 'shu']\n",
            "['nevertheless', 'buddhist', 'dharma', 'teaching', 'laozi', 'confucius', 'separate', 'tenor', 'entirely', 'go', 'terms', 'ethical', 'education']\n",
            "['right', 'awakening', 'thus', 'accomplished', 'buddha', 'appeared', 'world']\n",
            "['english', 'translation', 'john', 'mcrae', 'sutra', 'queen', 'srimala', 'lion', 'vimalakirti', 'sutra', 'berkeley', 'numata', 'center', 'buddhist', 'translation', 'research', 'pp']\n",
            "['buddha', 'said', 'śakra', 'lord', 'gods', 'acquired', 'new', 'blissful', 'joy', 'memorable', 'happiness', 'result', 'expect', 'ﬁnd', 'canonical', 'book', 'buddha', 'lengthy', 'discourses', 'volume', 'ii', 'śakra', 'lord', 'gods', 'said', 'buddha', 'sought', 'ﬁve', 'kinds', 'meritorious', 'results', 'state', 'blissful', 'joy', 'memorable', 'happiness']\n",
            "['examples', 'show', 'slippage', 'pronunciations']\n",
            "['veriﬁed', 'kinds', 'falsehoods']\n",
            "['laozi', 'said', 'even', 'heaven', 'earth', 'last', 'long']\n",
            "['destroyed', 'old', 'age', 'death', 'far', 'removed', 'noble', 'quality']\n",
            "['become', 'upāsikā', 'laywoman', 'order', 'devote', 'right', 'dharma', 'end', 'life', 'keep', 'vow', 'injure', 'life', 'steal', 'commit', 'sexual', 'misconduct', 'tell', 'lies', 'take', 'intoxicants', 'sir']\n",
            "['immediately', 'took', 'placed', 'palm']\n",
            "['case', 'illness', 'necessary', 'wear', 'record', 'inner', 'law', 'sent', 'home', 'south', 'seas', 'clothes', 'one', 'may', 'one', 'discretion', 'one', 'act', 'rules']\n",
            "['seen', 'ﬁgure', 'son', 'gone', 'forth', 'ruining', 'appearance']\n",
            "['way', 'morality', 'known']\n",
            "['weimojie', 'suoshuo', 'jing', '維摩詰所説經', 'skt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkresults(Buddhism_sentences_nl)"
      ],
      "metadata": {
        "id": "oOdyYbWwG-Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "savesentences('Buddhism_sentences', Buddhism_sentences)"
      ],
      "metadata": {
        "id": "urtA5lxvHAJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "savesentencesnl('Buddhism_sentences', Buddhism_sentences)"
      ],
      "metadata": {
        "id": "xk2HKkubbrZi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}